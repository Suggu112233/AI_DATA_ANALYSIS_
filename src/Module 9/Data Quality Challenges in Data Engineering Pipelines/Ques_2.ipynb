{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1: Handling Schema Mismatches using Spark\n",
    "**Description**: Use Apache Spark to address schema mismatches by transforming data to match\n",
    "the expected schema.\n",
    "\n",
    "**Steps**:\n",
    "1. Create Spark session\n",
    "2. Load dataframe\n",
    "3. Define the expected schema\n",
    "4. Handle schema mismatches\n",
    "5. Show corrected data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "JAVA_HOME is not set\n"
     ]
    },
    {
     "ename": "PySparkRuntimeError",
     "evalue": "[JAVA_GATEWAY_EXITED] Java gateway process exited before sending its port number.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPySparkRuntimeError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 12\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpyspark\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msql\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunctions\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m col\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# -----------------------------------------------------------------------------\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Step 1: Create Spark session\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# -----------------------------------------------------------------------------\u001b[39;00m\n\u001b[1;32m     10\u001b[0m spark \u001b[38;5;241m=\u001b[39m \u001b[43mSparkSession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuilder\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mappName\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mSchemaMismatchHandler\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[0;32m---> 12\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetOrCreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# -----------------------------------------------------------------------------\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# Step 2: Simulate data with schema mismatches (Create CSV manually in code)\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# -----------------------------------------------------------------------------\u001b[39;00m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# Create a sample CSV file with incorrect types\u001b[39;00m\n\u001b[1;32m     18\u001b[0m csv_data \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\"\"\u001b[39m\u001b[38;5;124mid,name,age,salary,is_active\u001b[39m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;124m1,Alice,30,70000.0,true\u001b[39m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;124m2,Bob,twenty-five,55000.5,false\u001b[39m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;124m3,Charlie,28,\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msixty thousand\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,yes\u001b[39m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;124m4,David,35,60000.0,true\u001b[39m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;124m\"\"\"\u001b[39m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pyspark/sql/session.py:556\u001b[0m, in \u001b[0;36mSparkSession.Builder.getOrCreate\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    554\u001b[0m     sparkConf\u001b[38;5;241m.\u001b[39mset(key, value)\n\u001b[1;32m    555\u001b[0m \u001b[38;5;66;03m# This SparkContext may be an existing one.\u001b[39;00m\n\u001b[0;32m--> 556\u001b[0m sc \u001b[38;5;241m=\u001b[39m \u001b[43mSparkContext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetOrCreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43msparkConf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    557\u001b[0m \u001b[38;5;66;03m# Do not update `SparkConf` for existing `SparkContext`, as it's shared\u001b[39;00m\n\u001b[1;32m    558\u001b[0m \u001b[38;5;66;03m# by all sessions.\u001b[39;00m\n\u001b[1;32m    559\u001b[0m session \u001b[38;5;241m=\u001b[39m SparkSession(sc, options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_options)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pyspark/core/context.py:523\u001b[0m, in \u001b[0;36mSparkContext.getOrCreate\u001b[0;34m(cls, conf)\u001b[0m\n\u001b[1;32m    521\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[1;32m    522\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_active_spark_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 523\u001b[0m         \u001b[43mSparkContext\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconf\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mSparkConf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    524\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_active_spark_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    525\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_active_spark_context\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pyspark/core/context.py:205\u001b[0m, in \u001b[0;36mSparkContext.__init__\u001b[0;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls, udf_profiler_cls, memory_profiler_cls)\u001b[0m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m gateway \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m gateway\u001b[38;5;241m.\u001b[39mgateway_parameters\u001b[38;5;241m.\u001b[39mauth_token \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    200\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    201\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou are trying to pass an insecure Py4j gateway to Spark. This\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    202\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m is not allowed as it is a security risk.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    203\u001b[0m     )\n\u001b[0;32m--> 205\u001b[0m \u001b[43mSparkContext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_ensure_initialized\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgateway\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgateway\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    206\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    207\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_do_init(\n\u001b[1;32m    208\u001b[0m         master,\n\u001b[1;32m    209\u001b[0m         appName,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    219\u001b[0m         memory_profiler_cls,\n\u001b[1;32m    220\u001b[0m     )\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pyspark/core/context.py:444\u001b[0m, in \u001b[0;36mSparkContext._ensure_initialized\u001b[0;34m(cls, instance, gateway, conf)\u001b[0m\n\u001b[1;32m    442\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[1;32m    443\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_gateway:\n\u001b[0;32m--> 444\u001b[0m         SparkContext\u001b[38;5;241m.\u001b[39m_gateway \u001b[38;5;241m=\u001b[39m gateway \u001b[38;5;129;01mor\u001b[39;00m \u001b[43mlaunch_gateway\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    445\u001b[0m         SparkContext\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;241m=\u001b[39m SparkContext\u001b[38;5;241m.\u001b[39m_gateway\u001b[38;5;241m.\u001b[39mjvm\n\u001b[1;32m    447\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m instance:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pyspark/java_gateway.py:111\u001b[0m, in \u001b[0;36mlaunch_gateway\u001b[0;34m(conf, popen_kwargs)\u001b[0m\n\u001b[1;32m    108\u001b[0m     time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m0.1\u001b[39m)\n\u001b[1;32m    110\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39misfile(conn_info_file):\n\u001b[0;32m--> 111\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m PySparkRuntimeError(\n\u001b[1;32m    112\u001b[0m         errorClass\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mJAVA_GATEWAY_EXITED\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    113\u001b[0m         messageParameters\u001b[38;5;241m=\u001b[39m{},\n\u001b[1;32m    114\u001b[0m     )\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(conn_info_file, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m info:\n\u001b[1;32m    117\u001b[0m     gateway_port \u001b[38;5;241m=\u001b[39m read_int(info)\n",
      "\u001b[0;31mPySparkRuntimeError\u001b[0m: [JAVA_GATEWAY_EXITED] Java gateway process exited before sending its port number."
     ]
    }
   ],
   "source": [
    "# handle_schema_mismatches.py\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, FloatType, BooleanType\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Step 1: Create Spark session\n",
    "# -----------------------------------------------------------------------------\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"SchemaMismatchHandler\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Step 2: Simulate data with schema mismatches (Create CSV manually in code)\n",
    "# -----------------------------------------------------------------------------\n",
    "# Create a sample CSV file with incorrect types\n",
    "csv_data = \"\"\"id,name,age,salary,is_active\n",
    "1,Alice,30,70000.0,true\n",
    "2,Bob,twenty-five,55000.5,false\n",
    "3,Charlie,28,\"sixty thousand\",yes\n",
    "4,David,35,60000.0,true\n",
    "\"\"\"\n",
    "\n",
    "with open(\"schema_mismatch.csv\", \"w\") as f:\n",
    "    f.write(csv_data)\n",
    "\n",
    "# Load the data with automatic schema inference (may be wrong)\n",
    "raw_df = spark.read.csv(\"schema_mismatch.csv\", header=True, inferSchema=True)\n",
    "\n",
    "print(\"üö´ Raw Data with Schema Mismatches:\")\n",
    "raw_df.show(truncate=False)\n",
    "raw_df.printSchema()\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Step 3: Define the expected schema\n",
    "# -----------------------------------------------------------------------------\n",
    "expected_schema = StructType([\n",
    "    StructField(\"id\", IntegerType(), True),\n",
    "    StructField(\"name\", StringType(), True),\n",
    "    StructField(\"age\", IntegerType(), True),\n",
    "    StructField(\"salary\", FloatType(), True),\n",
    "    StructField(\"is_active\", BooleanType(), True)\n",
    "])\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Step 4: Handle schema mismatches (Transform types to match expected schema)\n",
    "# -----------------------------------------------------------------------------\n",
    "# Cast columns to match expected schema\n",
    "corrected_df = raw_df \\\n",
    "    .withColumn(\"id\", col(\"id\").cast(\"int\")) \\\n",
    "    .withColumn(\"name\", col(\"name\").cast(\"string\")) \\\n",
    "    .withColumn(\"age\", col(\"age\").cast(\"int\")) \\\n",
    "    .withColumn(\"salary\", col(\"salary\").cast(\"float\")) \\\n",
    "    .withColumn(\"is_active\", col(\"is_active\").cast(\"boolean\"))\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Step 5: Show corrected data\n",
    "# -----------------------------------------------------------------------------\n",
    "print(\"‚úÖ Corrected DataFrame:\")\n",
    "corrected_df.show(truncate=False)\n",
    "corrected_df.printSchema()\n",
    "\n",
    "# Optional: Handle invalid rows (e.g., nulls after cast)\n",
    "invalid_rows = corrected_df.filter(\n",
    "    col(\"age\").isNull() | col(\"salary\").isNull() | col(\"is_active\").isNull()\n",
    ")\n",
    "\n",
    "print(\"‚ö†Ô∏è Rows with unresolved schema mismatches (nulls after casting):\")\n",
    "invalid_rows.show(truncate=False)\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Stop Spark session\n",
    "# -----------------------------------------------------------------------------\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2: Detect and Correct Incomplete Data in ETL\n",
    "**Description**: Use Python and Pandas to detect incomplete data in an ETL process and fill\n",
    "missing values with estimates.\n",
    "\n",
    "**Steps**:\n",
    "1. Detect incomplete data\n",
    "2. Fill missing values\n",
    "3. Report changes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pyspark'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Write your code from here\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpyspark\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msql\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m SparkSession\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpyspark\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msql\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtypes\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m StructType, StructField, IntegerType, StringType, DoubleType\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpyspark\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msql\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunctions\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m col\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pyspark'"
     ]
    }
   ],
   "source": [
    "# etl_detect_and_correct.py\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Step 0: Create a sample CSV file with incomplete data\n",
    "# -----------------------------------------------------------------------------\n",
    "sample_csv = \"\"\"\\\n",
    "OrderID,Customer,Quantity,UnitPrice,Region\n",
    "1001,Alice,5,20.0,North\n",
    "1002,Bob,,25.0,East\n",
    "1003,Charlie,3,,West\n",
    "1004,,2,15.0,South\n",
    "1005,Eve,7,22.5,\n",
    "\"\"\"\n",
    "\n",
    "with open('etl_incomplete.csv', 'w') as f:\n",
    "    f.write(sample_csv)\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Step 1: Load data and detect incomplete entries\n",
    "# -----------------------------------------------------------------------------\n",
    "df = pd.read_csv('etl_incomplete.csv')\n",
    "print(\"üì• Loaded Data:\")\n",
    "print(df, \"\\n\")\n",
    "\n",
    "missing_counts = df.isnull().sum()\n",
    "print(\"‚ùó Missing values per column:\")\n",
    "print(missing_counts, \"\\n\")\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Step 2: Fill missing values with estimates\n",
    "# -----------------------------------------------------------------------------\n",
    "# We'll compute fill-values once so we can report them:\n",
    "fills = {}\n",
    "\n",
    "# Numeric: Quantity ‚Üí fill with median\n",
    "median_qty = df['Quantity'].median()\n",
    "fills['Quantity'] = median_qty\n",
    "df['Quantity'] = df['Quantity'].fillna(median_qty)\n",
    "\n",
    "# Numeric: UnitPrice ‚Üí fill with mean\n",
    "mean_price = df['UnitPrice'].mean()\n",
    "fills['UnitPrice'] = mean_price\n",
    "df['UnitPrice'] = df['UnitPrice'].fillna(mean_price)\n",
    "\n",
    "# Categorical: Customer ‚Üí fill with a placeholder \"Unknown\"\n",
    "fills['Customer'] = \"Unknown\"\n",
    "df['Customer'] = df['Customer'].fillna(\"Unknown\")\n",
    "\n",
    "# Categorical: Region ‚Üí fill with mode (most frequent), or \"Unknown\" if tie/NaN\n",
    "if df['Region'].mode().size > 0:\n",
    "    mode_region = df['Region'].mode()[0]\n",
    "else:\n",
    "    mode_region = \"Unknown\"\n",
    "fills['Region'] = mode_region\n",
    "df['Region'] = df['Region'].fillna(mode_region)\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Step 3: Report exactly which cells were imputed\n",
    "# -----------------------------------------------------------------------------\n",
    "print(\"üõ†Ô∏è Imputation summary:\")\n",
    "for col, fill_val in fills.items():\n",
    "    # find original null positions by re-reading\n",
    "    orig = pd.read_csv('etl_incomplete.csv')[col]\n",
    "    null_idxs = orig[orig.isnull()].index.tolist()\n",
    "    if null_idxs:\n",
    "        print(f\" ‚Ä¢ Column '{col}': filled {len(null_idxs)} missing value(s) at rows {null_idxs} with ‚Üí {fill_val}\")\n",
    "    else:\n",
    "        print(f\" ‚Ä¢ Column '{col}': no missing values to fill\")\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Show the corrected DataFrame\n",
    "# -----------------------------------------------------------------------------\n",
    "print(\"\\n‚úÖ Corrected DataFrame:\")\n",
    "print(df)\n",
    "\n",
    "# Optionally, save the cleaned DataFrame\n",
    "df.to_csv('etl_incomplete_cleaned.csv', index=False)\n",
    "print(\"\\nüíæ Cleaned data saved to 'etl_incomplete_cleaned.csv'\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
