{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1: Detecting Missing Values during Data Ingestion\n",
    "**Description**: You have a CSV file with missing values in some columns. Write a Python script to detect and report missing values during the ingestion process.\n",
    "\n",
    "**Steps**:\n",
    "1. Load data\n",
    "2. Check for missing values\n",
    "3. Report missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❗ Missing values detected during data ingestion:\n",
      "Error: 'Series' object has no attribute 'iteritems'\n"
     ]
    }
   ],
   "source": [
    "# detect_missing_ingestion.py\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Step 0: Create a sample CSV file with missing values\n",
    "# -----------------------------------------------------------------------------\n",
    "sample_csv = \"\"\"ID,Name,Age,Salary\n",
    "1,John,28,50000\n",
    "2,Jane,,60000\n",
    "3,Bob,22,\n",
    "4,Alice,30,58000\n",
    "5,,25,52000\n",
    "\"\"\"\n",
    "\n",
    "with open('sample.csv', 'w') as f:\n",
    "    f.write(sample_csv)\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Step 1 & 2: Load data and check for missing values\n",
    "# -----------------------------------------------------------------------------\n",
    "def load_and_check(file_path: str) -> pd.DataFrame:\n",
    "    # Load the CSV into a DataFrame\n",
    "    df = pd.read_csv(file_path)\n",
    "    \n",
    "    # Count missing values per column\n",
    "    missing_counts = df.isnull().sum()\n",
    "    total_missing = missing_counts.sum()\n",
    "    \n",
    "    # Step 3: Report (and error) if any missing values found\n",
    "    if total_missing > 0:\n",
    "        print(\"❗ Missing values detected during data ingestion:\")\n",
    "        for col, cnt in missing_counts.iteritems():\n",
    "            if cnt > 0:\n",
    "                print(f\"  • Column '{col}': {cnt} missing value(s)\")\n",
    "        raise ValueError(f\"Data ingestion halted: {total_missing} total missing value(s) found.\")\n",
    "    \n",
    "    print(\"✅ No missing values found. Data ingestion successful.\")\n",
    "    return df\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Main entrypoint\n",
    "# -----------------------------------------------------------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        df = load_and_check('sample.csv')\n",
    "        # If no exception, proceed with further processing...\n",
    "        # e.g. train model, save to database, etc.\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        # Optionally, sys.exit(1) or other error handling here.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2: Validate Data Types during Extraction\n",
    "**Description**: You have a JSON file that should have specific data types for each field. Write a script to validate if the data types match the expected schema.\n",
    "\n",
    "**Steps**:\n",
    "1. Define expected schema\n",
    "2. Validate data types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❗ Data type validation errors detected:\n",
      "  - Record 2, field 'age': expected int, got str\n",
      "  - Record 3, field 'name': expected str, got int\n",
      "  - Record 3, field 'salary': expected float, got str\n",
      "  - Record 3, field 'is_active': expected bool, got str\n",
      "Error: 4 data type error(s) found. Extraction halted.\n"
     ]
    }
   ],
   "source": [
    "# validate_data_types.py\n",
    "\n",
    "import json\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Step 0: Create a sample JSON file with intentional type mismatches\n",
    "# -----------------------------------------------------------------------------\n",
    "sample_data = [\n",
    "    {\"id\": 1, \"name\": \"Alice\",   \"age\": 30,    \"salary\": 70000.0, \"is_active\": True},\n",
    "    {\"id\": 2, \"name\": \"Bob\",     \"age\": \"27\",  \"salary\": 55000.5, \"is_active\": False},  # age as str\n",
    "    {\"id\": 3, \"name\": 12345,     \"age\": 22,    \"salary\": \"62000\", \"is_active\": \"yes\"},  # name & salary & is_active wrong\n",
    "    {\"id\": 4, \"name\": \"Charlie\", \"age\": 28,    \"salary\": 60000.0, \"is_active\": True}\n",
    "]\n",
    "\n",
    "with open('sample.json', 'w') as f:\n",
    "    json.dump(sample_data, f, indent=2)\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Step 1: Define the expected schema\n",
    "# -----------------------------------------------------------------------------\n",
    "# Map field names to expected Python types\n",
    "EXPECTED_SCHEMA = {\n",
    "    \"id\": int,\n",
    "    \"name\": str,\n",
    "    \"age\": int,\n",
    "    \"salary\": float,\n",
    "    \"is_active\": bool\n",
    "}\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Step 2: Load and validate\n",
    "# -----------------------------------------------------------------------------\n",
    "def load_and_validate(file_path: str, schema: dict):\n",
    "    with open(file_path, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    errors = []\n",
    "    for idx, record in enumerate(data, start=1):\n",
    "        for field, expected_type in schema.items():\n",
    "            if field not in record:\n",
    "                errors.append(f\"Record {idx}: Missing field '{field}'.\")\n",
    "                continue\n",
    "            \n",
    "            value = record[field]\n",
    "            # Special handling: allow ints where floats expected\n",
    "            if expected_type is float and isinstance(value, int):\n",
    "                # auto-coerce is fine, but we still note it\n",
    "                continue\n",
    "            if not isinstance(value, expected_type):\n",
    "                errors.append(\n",
    "                    f\"Record {idx}, field '{field}': \"\n",
    "                    f\"expected {expected_type.__name__}, got {type(value).__name__}\"\n",
    "                )\n",
    "    if errors:\n",
    "        print(\"❗ Data type validation errors detected:\")\n",
    "        for err in errors:\n",
    "            print(\"  -\", err)\n",
    "        raise ValueError(f\"{len(errors)} data type error(s) found. Extraction halted.\")\n",
    "    \n",
    "    print(\"✅ All records match the expected schema!\")\n",
    "    return data\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Main\n",
    "# -----------------------------------------------------------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        records = load_and_validate('sample.json', EXPECTED_SCHEMA)\n",
    "        # proceed with downstream processing...\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        # optionally exit with non-zero code\n",
    "        # import sys; sys.exit(1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3: Remove Duplicate Records in Data\n",
    "**Description**: You have a dataset with duplicate entries. Write a Python script to find and remove duplicate records using Pandas.\n",
    "\n",
    "**Steps**:\n",
    "1. Find duplicate records\n",
    "2. Remove duplicates\n",
    "3. Report results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total rows loaded: 7\n",
      "Number of duplicate rows (including repeats): 4\n",
      "\n",
      "Duplicate records:\n",
      " ID Name  Age  Salary\n",
      "  2 Jane   32   60000\n",
      "  3  Bob   22   45000\n",
      "  2 Jane   32   60000\n",
      "  3  Bob   22   45000\n",
      "\n",
      "Duplicates removed: 2\n",
      "Rows after deduplication: 5\n",
      "\n",
      "Cleaned data saved to 'sample_duplicates_clean.csv'.\n"
     ]
    }
   ],
   "source": [
    "# remove_duplicates.py\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Step 0: Create a sample CSV file with duplicate records\n",
    "# -----------------------------------------------------------------------------\n",
    "sample_csv = \"\"\"\\\n",
    "ID,Name,Age,Salary\n",
    "1,John,28,50000\n",
    "2,Jane,32,60000\n",
    "3,Bob,22,45000\n",
    "2,Jane,32,60000\n",
    "4,Alice,30,58000\n",
    "3,Bob,22,45000\n",
    "5,Charlie,25,52000\n",
    "\"\"\"\n",
    "\n",
    "with open('sample_duplicates.csv', 'w') as f:\n",
    "    f.write(sample_csv)\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Step 1: Load data and find duplicates\n",
    "# -----------------------------------------------------------------------------\n",
    "def load_and_find_duplicates(file_path: str) -> pd.DataFrame:\n",
    "    df = pd.read_csv(file_path)\n",
    "    total_rows = len(df)\n",
    "    \n",
    "    # Identify duplicated rows (all columns)\n",
    "    dup_mask = df.duplicated(keep=False)\n",
    "    duplicates = df[dup_mask]\n",
    "    num_duplicates = len(duplicates)\n",
    "    \n",
    "    print(f\"Total rows loaded: {total_rows}\")\n",
    "    print(f\"Number of duplicate rows (including repeats): {num_duplicates}\")\n",
    "    if num_duplicates > 0:\n",
    "        print(\"\\nDuplicate records:\")\n",
    "        print(duplicates.to_string(index=False))\n",
    "    else:\n",
    "        print(\"No duplicates found.\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Step 2: Remove duplicates and report results\n",
    "# -----------------------------------------------------------------------------\n",
    "def remove_duplicates(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    # By default, keep='first' keeps the first occurrence and drops later ones\n",
    "    df_clean = df.drop_duplicates(keep='first').reset_index(drop=True)\n",
    "    removed = len(df) - len(df_clean)\n",
    "    \n",
    "    print(f\"\\nDuplicates removed: {removed}\")\n",
    "    print(f\"Rows after deduplication: {len(df_clean)}\")\n",
    "    return df_clean\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Main\n",
    "# -----------------------------------------------------------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    # Load and report duplicates\n",
    "    df = load_and_find_duplicates('sample_duplicates.csv')\n",
    "    \n",
    "    # Remove duplicates\n",
    "    df_clean = remove_duplicates(df)\n",
    "    \n",
    "    # Optionally, save the cleaned DataFrame\n",
    "    df_clean.to_csv('sample_duplicates_clean.csv', index=False)\n",
    "    print(\"\\nCleaned data saved to 'sample_duplicates_clean.csv'.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
