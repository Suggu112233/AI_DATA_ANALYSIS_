{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ensuring Feature Consistency Between Training & InferencePipelines:\n",
    "\n",
    "**Task 1**: Consistent Feature Preparation\n",
    "- Step 1: Write a function for data preprocessing and imputation shared by both training and inference pipelines.\n",
    "- Step 2: Demonstrate consistent application on both datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed training data:\n",
      "[[-1.5 -1.5 -1.5]\n",
      " [-0.5  0.5  0.5]\n",
      " [ 0.  -0.5 -0.5]\n",
      " [ 1.5  0.   1.5]\n",
      " [ 0.5  1.5  0. ]]\n",
      "\n",
      "Processed inference data:\n",
      "[[-0.9 -1.1 -1. ]\n",
      " [ 0.   0.9  0. ]\n",
      " [ 3.5  0.   1. ]]\n"
     ]
    }
   ],
   "source": [
    "# write your code from here\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Sample training data with missing values\n",
    "train_data = pd.DataFrame({\n",
    "    'age': [25, 30, np.nan, 40, 35],\n",
    "    'income': [50000, 60000, 55000, np.nan, 65000],\n",
    "    'score': [200, 220, 210, 230, np.nan]\n",
    "})\n",
    "\n",
    "# Sample inference data with missing values\n",
    "inference_data = pd.DataFrame({\n",
    "    'age': [28, np.nan, 50],\n",
    "    'income': [52000, 62000, np.nan],\n",
    "    'score': [205, 215, 225]\n",
    "})\n",
    "\n",
    "# Step 1: Shared preprocessing function\n",
    "def preprocess_data(df, imputer=None, scaler=None, fit=True):\n",
    "    \"\"\"\n",
    "    Preprocesses the input DataFrame by imputing missing values and scaling features.\n",
    "    \n",
    "    Parameters:\n",
    "    - df: pd.DataFrame, input data\n",
    "    - imputer: sklearn SimpleImputer instance or None\n",
    "    - scaler: sklearn StandardScaler instance or None\n",
    "    - fit: bool, whether to fit imputer and scaler on this data\n",
    "    \n",
    "    Returns:\n",
    "    - processed_df: np.ndarray, transformed data\n",
    "    - imputer: fitted SimpleImputer instance\n",
    "    - scaler: fitted StandardScaler instance\n",
    "    \"\"\"\n",
    "    numeric_cols = df.columns\n",
    "    \n",
    "    if imputer is None:\n",
    "        imputer = SimpleImputer(strategy='mean')\n",
    "    if scaler is None:\n",
    "        scaler = StandardScaler()\n",
    "        \n",
    "    if fit:\n",
    "        # Fit imputer and scaler on training data\n",
    "        imputed_data = imputer.fit_transform(df[numeric_cols])\n",
    "        scaled_data = scaler.fit_transform(imputed_data)\n",
    "    else:\n",
    "        # Use existing imputer and scaler on inference data\n",
    "        imputed_data = imputer.transform(df[numeric_cols])\n",
    "        scaled_data = scaler.transform(imputed_data)\n",
    "    \n",
    "    return scaled_data, imputer, scaler\n",
    "\n",
    "\n",
    "# Step 2: Apply preprocessing consistently\n",
    "\n",
    "# Preprocess training data (fit=True)\n",
    "X_train_processed, fitted_imputer, fitted_scaler = preprocess_data(train_data, fit=True)\n",
    "\n",
    "print(\"Processed training data:\")\n",
    "print(X_train_processed)\n",
    "\n",
    "# Preprocess inference data using the fitted imputer and scaler (fit=False)\n",
    "X_infer_processed, _, _ = preprocess_data(inference_data, imputer=fitted_imputer, scaler=fitted_scaler, fit=False)\n",
    "\n",
    "print(\"\\nProcessed inference data:\")\n",
    "print(X_infer_processed)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 2**: Pipeline Integration\n",
    "- Step 1: Use sklearn pipelines to encapsulate the preprocessing steps.\n",
    "- Step 2: Configure identical pipelines for both training and building inference models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed training data:\n",
      "[[-1.5 -1.5 -1.5]\n",
      " [-0.5  0.5  0.5]\n",
      " [ 0.  -0.5 -0.5]\n",
      " [ 1.5  0.   1.5]\n",
      " [ 0.5  1.5  0. ]]\n",
      "\n",
      "Processed inference data:\n",
      "[[-0.9 -1.1 -1. ]\n",
      " [ 0.   0.9  0. ]\n",
      " [ 3.5  0.   1. ]]\n"
     ]
    }
   ],
   "source": [
    "# write your code from here\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Sample training data with missing values\n",
    "train_data = pd.DataFrame({\n",
    "    'age': [25, 30, np.nan, 40, 35],\n",
    "    'income': [50000, 60000, 55000, np.nan, 65000],\n",
    "    'score': [200, 220, 210, 230, np.nan]\n",
    "})\n",
    "\n",
    "# Sample inference data with missing values\n",
    "inference_data = pd.DataFrame({\n",
    "    'age': [28, np.nan, 50],\n",
    "    'income': [52000, 62000, np.nan],\n",
    "    'score': [205, 215, 225]\n",
    "})\n",
    "\n",
    "# Step 1: Create sklearn pipeline for preprocessing\n",
    "preprocessing_pipeline = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='mean')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "# Step 2: Fit pipeline on training data and transform training data\n",
    "X_train_processed = preprocessing_pipeline.fit_transform(train_data)\n",
    "\n",
    "print(\"Processed training data:\")\n",
    "print(X_train_processed)\n",
    "\n",
    "# Step 3: Use the same fitted pipeline to transform inference data\n",
    "X_infer_processed = preprocessing_pipeline.transform(inference_data)\n",
    "\n",
    "print(\"\\nProcessed inference data:\")\n",
    "print(X_infer_processed)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 3**: Saving and Loading Preprocessing Models\n",
    "- Step 1: Save the transformation model after fitting it to the training data.\n",
    "- Step 2: Load and apply the saved model during inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing pipeline saved to 'preprocessing_pipeline.joblib'\n",
      "Preprocessing pipeline loaded.\n",
      "\n",
      "Processed inference data using loaded pipeline:\n",
      "[[-0.9 -1.1 -1. ]\n",
      " [ 0.   0.9  0. ]\n",
      " [ 3.5  0.   1. ]]\n"
     ]
    }
   ],
   "source": [
    "# write your code from here\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "import joblib  # For saving and loading models\n",
    "\n",
    "# Sample training data\n",
    "train_data = pd.DataFrame({\n",
    "    'age': [25, 30, np.nan, 40, 35],\n",
    "    'income': [50000, 60000, 55000, np.nan, 65000],\n",
    "    'score': [200, 220, 210, 230, np.nan]\n",
    "})\n",
    "\n",
    "# Sample inference data\n",
    "inference_data = pd.DataFrame({\n",
    "    'age': [28, np.nan, 50],\n",
    "    'income': [52000, 62000, np.nan],\n",
    "    'score': [205, 215, 225]\n",
    "})\n",
    "\n",
    "# Step 1: Create pipeline\n",
    "preprocessing_pipeline = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='mean')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "# Step 2: Fit pipeline on training data\n",
    "preprocessing_pipeline.fit(train_data)\n",
    "\n",
    "# Step 3: Save the fitted pipeline to disk\n",
    "joblib.dump(preprocessing_pipeline, 'preprocessing_pipeline.joblib')\n",
    "print(\"Preprocessing pipeline saved to 'preprocessing_pipeline.joblib'\")\n",
    "\n",
    "# --- Later or in a different script ---\n",
    "\n",
    "# Step 4: Load the saved pipeline\n",
    "loaded_pipeline = joblib.load('preprocessing_pipeline.joblib')\n",
    "print(\"Preprocessing pipeline loaded.\")\n",
    "\n",
    "# Step 5: Apply loaded pipeline on inference data\n",
    "X_infer_processed = loaded_pipeline.transform(inference_data)\n",
    "\n",
    "print(\"\\nProcessed inference data using loaded pipeline:\")\n",
    "print(X_infer_processed)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
