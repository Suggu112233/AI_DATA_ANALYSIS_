{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handling Missing Values in Large-scale ML Pipelines:\n",
    "\n",
    "**Task 1**: Impute with Mean or Median\n",
    "- Step 1: Load a dataset with missing values (e.g., Boston Housing dataset).\n",
    "- Step 2: Identify columns with missing values.\n",
    "- Step 3: Impute missing values using the mean or median of the respective columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write your code from here\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.datasets import load_boston\n",
    "\n",
    "# Load Boston Housing dataset (deprecated in sklearn 1.2+, so alternatively create similar data)\n",
    "# We'll create a DataFrame from the dataset\n",
    "boston = load_boston()\n",
    "df = pd.DataFrame(boston.data, columns=boston.feature_names)\n",
    "df['TARGET'] = boston.target\n",
    "\n",
    "# Introduce missing values randomly for demonstration\n",
    "np.random.seed(42)\n",
    "missing_rate = 0.1\n",
    "n_missing_samples = int(np.floor(missing_rate * df.shape[0]))\n",
    "\n",
    "for col in ['RM', 'AGE']:  # Pick two columns to insert missing values\n",
    "    missing_indices = np.random.choice(df.index, n_missing_samples, replace=False)\n",
    "    df.loc[missing_indices, col] = np.nan\n",
    "\n",
    "print(\"Original data with missing values:\")\n",
    "print(df[['RM', 'AGE']].head(10))\n",
    "\n",
    "# Step 2: Identify columns with missing values\n",
    "missing_cols = df.columns[df.isnull().any()].tolist()\n",
    "print(\"\\nColumns with missing values:\", missing_cols)\n",
    "\n",
    "# Step 3: Impute missing values using mean or median\n",
    "# Mean imputation for 'RM', Median imputation for 'AGE' (just as an example)\n",
    "imputer_mean = SimpleImputer(strategy='mean')\n",
    "imputer_median = SimpleImputer(strategy='median')\n",
    "\n",
    "df['RM'] = imputer_mean.fit_transform(df[['RM']])\n",
    "df['AGE'] = imputer_median.fit_transform(df[['AGE']])\n",
    "\n",
    "print(\"\\nData after imputation:\")\n",
    "print(df[['RM', 'AGE']].head(10))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 2**: Impute with the Most Frequent Value\n",
    "- Step 1: Use the Titanic dataset and identify columns with missing values.\n",
    "- Step 2: Impute categorical columns using the most frequent value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write your code from here\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Step 1: Load Titanic dataset\n",
    "titanic = sns.load_dataset('titanic')\n",
    "\n",
    "# Show columns with missing values\n",
    "missing_cols = titanic.columns[titanic.isnull().any()].tolist()\n",
    "print(\"Columns with missing values:\", missing_cols)\n",
    "\n",
    "# Identify categorical columns with missing values\n",
    "cat_cols = titanic.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "cat_cols_with_missing = [col for col in cat_cols if col in missing_cols]\n",
    "\n",
    "print(\"Categorical columns with missing values:\", cat_cols_with_missing)\n",
    "\n",
    "# Step 2: Impute categorical columns using the most frequent value (mode)\n",
    "imputer = SimpleImputer(strategy='most_frequent')\n",
    "\n",
    "# Apply imputer column-wise\n",
    "titanic[cat_cols_with_missing] = imputer.fit_transform(titanic[cat_cols_with_missing])\n",
    "\n",
    "# Verify missing values are filled\n",
    "print(\"\\nMissing values after imputation:\")\n",
    "print(titanic[cat_cols_with_missing].isnull().sum())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 3**: Advanced Imputation - k-Nearest Neighbors\n",
    "- Step 1: Implement KNN imputation using the KNNImputer from sklearn.\n",
    "- Step 2: Explore how KNN imputation improves data completion over simpler methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write your code from here\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn.impute import KNNImputer, SimpleImputer\n",
    "\n",
    "# Load Titanic dataset\n",
    "titanic = sns.load_dataset('titanic')\n",
    "\n",
    "# Select numerical columns for KNN imputation\n",
    "num_cols = titanic.select_dtypes(include=['float64', 'int64']).columns.tolist()\n",
    "\n",
    "# Show missing values in numerical columns before imputation\n",
    "print(\"Missing values in numerical columns before imputation:\")\n",
    "print(titanic[num_cols].isnull().sum())\n",
    "\n",
    "# Step 1: KNN Imputation (k=3 neighbors)\n",
    "knn_imputer = KNNImputer(n_neighbors=3)\n",
    "titanic_knn_imputed = titanic.copy()\n",
    "titanic_knn_imputed[num_cols] = knn_imputer.fit_transform(titanic[num_cols])\n",
    "\n",
    "print(\"\\nMissing values in numerical columns after KNN imputation:\")\n",
    "print(pd.DataFrame(titanic_knn_imputed[num_cols]).isnull().sum())\n",
    "\n",
    "# Step 2: Compare with SimpleImputer (mean strategy)\n",
    "simple_imputer = SimpleImputer(strategy='mean')\n",
    "titanic_simple_imputed = titanic.copy()\n",
    "titanic_simple_imputed[num_cols] = simple_imputer.fit_transform(titanic[num_cols])\n",
    "\n",
    "print(\"\\nMissing values in numerical columns after SimpleImputer (mean) imputation:\")\n",
    "print(pd.DataFrame(titanic_simple_imputed[num_cols]).isnull().sum())\n",
    "\n",
    "# Optional: Show some original vs imputed values for comparison\n",
    "print(\"\\nOriginal vs KNN Imputed (first 5 rows, 'age'):\")\n",
    "print(pd.DataFrame({'Original': titanic['age'].head(), 'KNN Imputed': titanic_knn_imputed['age'].head()}))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Scaling & Normalization Best Practices:\n",
    "\n",
    "**Task 1**: Standardization\n",
    "- Step 1: Standardize features using StandardScaler.\n",
    "- Step 2: Observe how standardization affects data distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write your code from here\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Load Titanic dataset\n",
    "titanic = sns.load_dataset('titanic')\n",
    "\n",
    "# Select numerical columns\n",
    "num_cols = titanic.select_dtypes(include=['float64', 'int64']).columns.tolist()\n",
    "\n",
    "# Drop rows with missing values in numerical columns for clean scaling\n",
    "titanic_num = titanic[num_cols].dropna()\n",
    "\n",
    "# Step 1: Standardize features using StandardScaler\n",
    "scaler = StandardScaler()\n",
    "titanic_scaled = scaler.fit_transform(titanic_num)\n",
    "\n",
    "# Convert back to DataFrame for easier viewing\n",
    "titanic_scaled_df = pd.DataFrame(titanic_scaled, columns=num_cols)\n",
    "\n",
    "# Step 2: Observe effect of standardization on data distribution\n",
    "\n",
    "# Plot original vs standardized feature distributions for 'age' and 'fare'\n",
    "fig, axs = plt.subplots(2, 2, figsize=(12, 8))\n",
    "\n",
    "# Original distributions\n",
    "sns.histplot(titanic_num['age'], kde=True, ax=axs[0,0], color='blue')\n",
    "axs[0,0].set_title('Original Age Distribution')\n",
    "\n",
    "sns.histplot(titanic_num['fare'], kde=True, ax=axs[0,1], color='blue')\n",
    "axs[0,1].set_title('Original Fare Distribution')\n",
    "\n",
    "# Standardized distributions\n",
    "sns.histplot(titanic_scaled_df['age'], kde=True, ax=axs[1,0], color='green')\n",
    "axs[1,0].set_title('Standardized Age Distribution')\n",
    "\n",
    "sns.histplot(titanic_scaled_df['fare'], kde=True, ax=axs[1,1], color='green')\n",
    "axs[1,1].set_title('Standardized Fare Distribution')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 2**: Min-Max Scaling\n",
    "\n",
    "- Step 1: Scale features to lie between 0 and 1 using MinMaxScaler.\n",
    "- Step 2: Compare with standardization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write your code from here\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "\n",
    "# Load Titanic dataset\n",
    "titanic = sns.load_dataset('titanic')\n",
    "\n",
    "# Select numerical columns\n",
    "num_cols = titanic.select_dtypes(include=['float64', 'int64']).columns.tolist()\n",
    "\n",
    "# Drop rows with missing values in numerical columns for clean scaling\n",
    "titanic_num = titanic[num_cols].dropna()\n",
    "\n",
    "# Standardization (for comparison)\n",
    "scaler_standard = StandardScaler()\n",
    "titanic_standardized = scaler_standard.fit_transform(titanic_num)\n",
    "titanic_standardized_df = pd.DataFrame(titanic_standardized, columns=num_cols)\n",
    "\n",
    "# Min-Max Scaling\n",
    "scaler_minmax = MinMaxScaler()\n",
    "titanic_minmax_scaled = scaler_minmax.fit_transform(titanic_num)\n",
    "titanic_minmax_df = pd.DataFrame(titanic_minmax_scaled, columns=num_cols)\n",
    "\n",
    "# Plotting original, standardized, and min-max scaled distributions for 'age' and 'fare'\n",
    "fig, axs = plt.subplots(3, 2, figsize=(12, 12))\n",
    "\n",
    "# Original distributions\n",
    "sns.histplot(titanic_num['age'], kde=True, ax=axs[0, 0], color='blue')\n",
    "axs[0, 0].set_title('Original Age Distribution')\n",
    "sns.histplot(titanic_num['fare'], kde=True, ax=axs[0, 1], color='blue')\n",
    "axs[0, 1].set_title('Original Fare Distribution')\n",
    "\n",
    "# Standardized distributions\n",
    "sns.histplot(titanic_standardized_df['age'], kde=True, ax=axs[1, 0], color='green')\n",
    "axs[1, 0].set_title('Standardized Age Distribution')\n",
    "sns.histplot(titanic_standardized_df['fare'], kde=True, ax=axs[1, 1], color='green')\n",
    "axs[1, 1].set_title('Standardized Fare Distribution')\n",
    "\n",
    "# Min-Max scaled distributions\n",
    "sns.histplot(titanic_minmax_df['age'], kde=True, ax=axs[2, 0], color='red')\n",
    "axs[2, 0].set_title('Min-Max Scaled Age Distribution')\n",
    "sns.histplot(titanic_minmax_df['fare'], kde=True, ax=axs[2, 1], color='red')\n",
    "axs[2, 1].set_title('Min-Max Scaled Fare Distribution')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 3**: Robust Scaling\n",
    "- Step 1: Scale features using RobustScaler, which is useful for data with outliers.\n",
    "- Step 2: Assess changes in data scaling compared to other scaling methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write your code from here\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler\n",
    "\n",
    "# Load Titanic dataset\n",
    "titanic = sns.load_dataset('titanic')\n",
    "\n",
    "# Select numerical columns\n",
    "num_cols = titanic.select_dtypes(include=['float64', 'int64']).columns.tolist()\n",
    "\n",
    "# Drop rows with missing values in numerical columns for clean scaling\n",
    "titanic_num = titanic[num_cols].dropna()\n",
    "\n",
    "# StandardScaler\n",
    "scaler_standard = StandardScaler()\n",
    "titanic_standardized = scaler_standard.fit_transform(titanic_num)\n",
    "titanic_standardized_df = pd.DataFrame(titanic_standardized, columns=num_cols)\n",
    "\n",
    "# MinMaxScaler\n",
    "scaler_minmax = MinMaxScaler()\n",
    "titanic_minmax_scaled = scaler_minmax.fit_transform(titanic_num)\n",
    "titanic_minmax_df = pd.DataFrame(titanic_minmax_scaled, columns=num_cols)\n",
    "\n",
    "# RobustScaler\n",
    "scaler_robust = RobustScaler()\n",
    "titanic_robust_scaled = scaler_robust.fit_transform(titanic_num)\n",
    "titanic_robust_df = pd.DataFrame(titanic_robust_scaled, columns=num_cols)\n",
    "\n",
    "# Plot distributions for 'fare' and 'age' to compare effects of scaling methods\n",
    "fig, axs = plt.subplots(4, 2, figsize=(12, 14))\n",
    "\n",
    "# Original distributions\n",
    "sns.histplot(titanic_num['age'], kde=True, ax=axs[0, 0], color='blue')\n",
    "axs[0, 0].set_title('Original Age Distribution')\n",
    "sns.histplot(titanic_num['fare'], kde=True, ax=axs[0, 1], color='blue')\n",
    "axs[0, 1].set_title('Original Fare Distribution')\n",
    "\n",
    "# Standardized distributions\n",
    "sns.histplot(titanic_standardized_df['age'], kde=True, ax=axs[1, 0], color='green')\n",
    "axs[1, 0].set_title('Standardized Age Distribution')\n",
    "sns.histplot(titanic_standardized_df['fare'], kde=True, ax=axs[1, 1], color='green')\n",
    "axs[1, 1].set_title('Standardized Fare Distribution')\n",
    "\n",
    "# Min-Max scaled distributions\n",
    "sns.histplot(titanic_minmax_df['age'], kde=True, ax=axs[2, 0], color='red')\n",
    "axs[2, 0].set_title('Min-Max Scaled Age Distribution')\n",
    "sns.histplot(titanic_minmax_df['fare'], kde=True, ax=axs[2, 1], color='red')\n",
    "axs[2, 1].set_title('Min-Max Scaled Fare Distribution')\n",
    "\n",
    "# Robust scaled distributions\n",
    "sns.histplot(titanic_robust_df['age'], kde=True, ax=axs[3, 0], color='purple')\n",
    "axs[3, 0].set_title('Robust Scaled Age Distributi_\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Selection Techniques:\n",
    "### Removing Highly Correlated Features:\n",
    "\n",
    "**Task 1**: Correlation Matrix\n",
    "- Step 1: Compute correlation matrix.\n",
    "- Step 2: Remove highly correlated features (correlation > 0.9)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# write your code from here\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_boston\n",
    "\n",
    "# Load dataset (Boston Housing for example)\n",
    "data = load_boston()\n",
    "df = pd.DataFrame(data.data, columns=data.feature_names)\n",
    "\n",
    "# Step 1: Compute correlation matrix\n",
    "corr_matrix = df.corr().abs()\n",
    "\n",
    "# Step 2: Select upper triangle of correlation matrix\n",
    "upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n",
    "\n",
    "# Find features with correlation greater than 0.9\n",
    "to_drop = [column for column in upper.columns if any(upper[column] > 0.9)]\n",
    "\n",
    "print(f\"Features to drop due to high correlation (> 0.9): {to_drop}\")\n",
    "\n",
    "# Drop highly correlated features\n",
    "df_reduced = df.drop(columns=to_drop)\n",
    "\n",
    "print(\"\\nRemaining features after removing highly correlated ones:\")\n",
    "print(df_reduced.columns.tolist())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Mutual Information & Variance Thresholds:\n",
    "\n",
    "**Task 2**: Mutual Information\n",
    "- Step 1: Compute mutual information between features and target.\n",
    "- Step 2: Retain features with high mutual information scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write your code from here\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "\n",
    "# Load example dataset\n",
    "data = load_breast_cancer()\n",
    "X = pd.DataFrame(data.data, columns=data.feature_names)\n",
    "y = data.target\n",
    "\n",
    "# Step 1: Compute mutual information between each feature and the target\n",
    "mi_scores = mutual_info_classif(X, y, discrete_features=False, random_state=42)\n",
    "\n",
    "# Create a DataFrame of features and their MI scores\n",
    "mi_df = pd.DataFrame({'Feature': X.columns, 'Mutual_Information': mi_scores})\n",
    "\n",
    "# Sort features by MI score descending\n",
    "mi_df = mi_df.sort_values(by='Mutual_Information', ascending=False)\n",
    "\n",
    "print(\"Mutual Information Scores:\")\n",
    "print(mi_df)\n",
    "\n",
    "# Step 2: Retain features with MI score above a threshold (e.g., > 0.01)\n",
    "threshold = 0.01\n",
    "selected_features = mi_df[mi_df['Mutual_Information'] > threshold]['Feature'].tolist()\n",
    "\n",
    "print(f\"\\nFeatures retained (MI > {threshold}):\")\n",
    "print(selected_features)\n",
    "\n",
    "# Reduced dataset with selected features\n",
    "X_reduced = X[selected_features]\n",
    "print(f\"\\nShape of reduced dataset: {X_reduced.shape}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 3**: Variance Threshold\n",
    "- Step 1: Implement VarianceThreshold to remove features with low variance.\n",
    "- Step 2: Analyze impact on feature space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write your code from here\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "\n",
    "# Load example dataset\n",
    "data = load_breast_cancer()\n",
    "X = pd.DataFrame(data.data, columns=data.feature_names)\n",
    "\n",
    "print(f\"Original number of features: {X.shape[1]}\")\n",
    "\n",
    "# Step 1: Remove features with low variance (threshold=0.01 here)\n",
    "selector = VarianceThreshold(threshold=0.01)\n",
    "X_reduced = selector.fit_transform(X)\n",
    "\n",
    "# Features kept mask\n",
    "features_kept = X.columns[selector.get_support()]\n",
    "\n",
    "print(f\"Number of features after VarianceThreshold: {X_reduced.shape[1]}\")\n",
    "print(\"Features retained:\")\n",
    "print(features_kept.tolist())\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
