{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handling Missing Values in Large-scale ML Pipelines:\n",
    "\n",
    "**Task 1**: Impute with Mean or Median\n",
    "- Step 1: Load a dataset with missing values (e.g., Boston Housing dataset).\n",
    "- Step 2: Identify columns with missing values.\n",
    "- Step 3: Impute missing values using the mean or median of the respective columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "\n`load_boston` has been removed from scikit-learn since version 1.2.\n\nThe Boston housing prices dataset has an ethical problem: as\ninvestigated in [1], the authors of this dataset engineered a\nnon-invertible variable \"B\" assuming that racial self-segregation had a\npositive impact on house prices [2]. Furthermore the goal of the\nresearch that led to the creation of this dataset was to study the\nimpact of air quality but it did not give adequate demonstration of the\nvalidity of this assumption.\n\nThe scikit-learn maintainers therefore strongly discourage the use of\nthis dataset unless the purpose of the code is to study and educate\nabout ethical issues in data science and machine learning.\n\nIn this special case, you can fetch the dataset from the original\nsource::\n\n    import pandas as pd\n    import numpy as np\n\n    data_url = \"http://lib.stat.cmu.edu/datasets/boston\"\n    raw_df = pd.read_csv(data_url, sep=\"\\s+\", skiprows=22, header=None)\n    data = np.hstack([raw_df.values[::2, :], raw_df.values[1::2, :2]])\n    target = raw_df.values[1::2, 2]\n\nAlternative datasets include the California housing dataset and the\nAmes housing dataset. You can load the datasets as follows::\n\n    from sklearn.datasets import fetch_california_housing\n    housing = fetch_california_housing()\n\nfor the California housing dataset and::\n\n    from sklearn.datasets import fetch_openml\n    housing = fetch_openml(name=\"house_prices\", as_frame=True)\n\nfor the Ames housing dataset.\n\n[1] M Carlisle.\n\"Racist data destruction?\"\n<https://medium.com/@docintangible/racist-data-destruction-113e3eff54a8>\n\n[2] Harrison Jr, David, and Daniel L. Rubinfeld.\n\"Hedonic housing prices and the demand for clean air.\"\nJournal of environmental economics and management 5.1 (1978): 81-102.\n<https://www.researchgate.net/publication/4974606_Hedonic_housing_prices_and_the_demand_for_clean_air>\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mimpute\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m SimpleImputer\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdatasets\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m load_boston\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Load Boston Housing dataset (deprecated in sklearn 1.2+, so alternatively create similar data)\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# We'll create a DataFrame from the dataset\u001b[39;00m\n\u001b[1;32m      9\u001b[0m boston \u001b[38;5;241m=\u001b[39m load_boston()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/datasets/__init__.py:161\u001b[0m, in \u001b[0;36m__getattr__\u001b[0;34m(name)\u001b[0m\n\u001b[1;32m    110\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mload_boston\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    111\u001b[0m     msg \u001b[38;5;241m=\u001b[39m textwrap\u001b[38;5;241m.\u001b[39mdedent(\n\u001b[1;32m    112\u001b[0m \u001b[38;5;250m        \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;124;03m        `load_boston` has been removed from scikit-learn since version 1.2.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    159\u001b[0m \u001b[38;5;124;03m        \"\"\"\u001b[39;00m\n\u001b[1;32m    160\u001b[0m     )\n\u001b[0;32m--> 161\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(msg)\n\u001b[1;32m    162\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    163\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mglobals\u001b[39m()[name]\n",
      "\u001b[0;31mImportError\u001b[0m: \n`load_boston` has been removed from scikit-learn since version 1.2.\n\nThe Boston housing prices dataset has an ethical problem: as\ninvestigated in [1], the authors of this dataset engineered a\nnon-invertible variable \"B\" assuming that racial self-segregation had a\npositive impact on house prices [2]. Furthermore the goal of the\nresearch that led to the creation of this dataset was to study the\nimpact of air quality but it did not give adequate demonstration of the\nvalidity of this assumption.\n\nThe scikit-learn maintainers therefore strongly discourage the use of\nthis dataset unless the purpose of the code is to study and educate\nabout ethical issues in data science and machine learning.\n\nIn this special case, you can fetch the dataset from the original\nsource::\n\n    import pandas as pd\n    import numpy as np\n\n    data_url = \"http://lib.stat.cmu.edu/datasets/boston\"\n    raw_df = pd.read_csv(data_url, sep=\"\\s+\", skiprows=22, header=None)\n    data = np.hstack([raw_df.values[::2, :], raw_df.values[1::2, :2]])\n    target = raw_df.values[1::2, 2]\n\nAlternative datasets include the California housing dataset and the\nAmes housing dataset. You can load the datasets as follows::\n\n    from sklearn.datasets import fetch_california_housing\n    housing = fetch_california_housing()\n\nfor the California housing dataset and::\n\n    from sklearn.datasets import fetch_openml\n    housing = fetch_openml(name=\"house_prices\", as_frame=True)\n\nfor the Ames housing dataset.\n\n[1] M Carlisle.\n\"Racist data destruction?\"\n<https://medium.com/@docintangible/racist-data-destruction-113e3eff54a8>\n\n[2] Harrison Jr, David, and Daniel L. Rubinfeld.\n\"Hedonic housing prices and the demand for clean air.\"\nJournal of environmental economics and management 5.1 (1978): 81-102.\n<https://www.researchgate.net/publication/4974606_Hedonic_housing_prices_and_the_demand_for_clean_air>\n"
     ]
    }
   ],
   "source": [
    "# write your code from here\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.datasets import load_boston\n",
    "\n",
    "# Load Boston Housing dataset (deprecated in sklearn 1.2+, so alternatively create similar data)\n",
    "# We'll create a DataFrame from the dataset\n",
    "boston = load_boston()\n",
    "df = pd.DataFrame(boston.data, columns=boston.feature_names)\n",
    "df['TARGET'] = boston.target\n",
    "\n",
    "# Introduce missing values randomly for demonstration\n",
    "np.random.seed(42)\n",
    "missing_rate = 0.1\n",
    "n_missing_samples = int(np.floor(missing_rate * df.shape[0]))\n",
    "\n",
    "for col in ['RM', 'AGE']:  # Pick two columns to insert missing values\n",
    "    missing_indices = np.random.choice(df.index, n_missing_samples, replace=False)\n",
    "    df.loc[missing_indices, col] = np.nan\n",
    "\n",
    "print(\"Original data with missing values:\")\n",
    "print(df[['RM', 'AGE']].head(10))\n",
    "\n",
    "# Step 2: Identify columns with missing values\n",
    "missing_cols = df.columns[df.isnull().any()].tolist()\n",
    "print(\"\\nColumns with missing values:\", missing_cols)\n",
    "\n",
    "# Step 3: Impute missing values using mean or median\n",
    "# Mean imputation for 'RM', Median imputation for 'AGE' (just as an example)\n",
    "imputer_mean = SimpleImputer(strategy='mean')\n",
    "imputer_median = SimpleImputer(strategy='median')\n",
    "\n",
    "df['RM'] = imputer_mean.fit_transform(df[['RM']])\n",
    "df['AGE'] = imputer_median.fit_transform(df[['AGE']])\n",
    "\n",
    "print(\"\\nData after imputation:\")\n",
    "print(df[['RM', 'AGE']].head(10))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 2**: Impute with the Most Frequent Value\n",
    "- Step 1: Use the Titanic dataset and identify columns with missing values.\n",
    "- Step 2: Impute categorical columns using the most frequent value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write your code from here\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Step 1: Load Titanic dataset\n",
    "titanic = sns.load_dataset('titanic')\n",
    "\n",
    "# Show columns with missing values\n",
    "missing_cols = titanic.columns[titanic.isnull().any()].tolist()\n",
    "print(\"Columns with missing values:\", missing_cols)\n",
    "\n",
    "# Identify categorical columns with missing values\n",
    "cat_cols = titanic.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "cat_cols_with_missing = [col for col in cat_cols if col in missing_cols]\n",
    "\n",
    "print(\"Categorical columns with missing values:\", cat_cols_with_missing)\n",
    "\n",
    "# Step 2: Impute categorical columns using the most frequent value (mode)\n",
    "imputer = SimpleImputer(strategy='most_frequent')\n",
    "\n",
    "# Apply imputer column-wise\n",
    "titanic[cat_cols_with_missing] = imputer.fit_transform(titanic[cat_cols_with_missing])\n",
    "\n",
    "# Verify missing values are filled\n",
    "print(\"\\nMissing values after imputation:\")\n",
    "print(titanic[cat_cols_with_missing].isnull().sum())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 3**: Advanced Imputation - k-Nearest Neighbors\n",
    "- Step 1: Implement KNN imputation using the KNNImputer from sklearn.\n",
    "- Step 2: Explore how KNN imputation improves data completion over simpler methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write your code from here\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn.impute import KNNImputer, SimpleImputer\n",
    "\n",
    "# Load Titanic dataset\n",
    "titanic = sns.load_dataset('titanic')\n",
    "\n",
    "# Select numerical columns for KNN imputation\n",
    "num_cols = titanic.select_dtypes(include=['float64', 'int64']).columns.tolist()\n",
    "\n",
    "# Show missing values in numerical columns before imputation\n",
    "print(\"Missing values in numerical columns before imputation:\")\n",
    "print(titanic[num_cols].isnull().sum())\n",
    "\n",
    "# Step 1: KNN Imputation (k=3 neighbors)\n",
    "knn_imputer = KNNImputer(n_neighbors=3)\n",
    "titanic_knn_imputed = titanic.copy()\n",
    "titanic_knn_imputed[num_cols] = knn_imputer.fit_transform(titanic[num_cols])\n",
    "\n",
    "print(\"\\nMissing values in numerical columns after KNN imputation:\")\n",
    "print(pd.DataFrame(titanic_knn_imputed[num_cols]).isnull().sum())\n",
    "\n",
    "# Step 2: Compare with SimpleImputer (mean strategy)\n",
    "simple_imputer = SimpleImputer(strategy='mean')\n",
    "titanic_simple_imputed = titanic.copy()\n",
    "titanic_simple_imputed[num_cols] = simple_imputer.fit_transform(titanic[num_cols])\n",
    "\n",
    "print(\"\\nMissing values in numerical columns after SimpleImputer (mean) imputation:\")\n",
    "print(pd.DataFrame(titanic_simple_imputed[num_cols]).isnull().sum())\n",
    "\n",
    "# Optional: Show some original vs imputed values for comparison\n",
    "print(\"\\nOriginal vs KNN Imputed (first 5 rows, 'age'):\")\n",
    "print(pd.DataFrame({'Original': titanic['age'].head(), 'KNN Imputed': titanic_knn_imputed['age'].head()}))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Scaling & Normalization Best Practices:\n",
    "\n",
    "**Task 1**: Standardization\n",
    "- Step 1: Standardize features using StandardScaler.\n",
    "- Step 2: Observe how standardization affects data distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write your code from here\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Load Titanic dataset\n",
    "titanic = sns.load_dataset('titanic')\n",
    "\n",
    "# Select numerical columns\n",
    "num_cols = titanic.select_dtypes(include=['float64', 'int64']).columns.tolist()\n",
    "\n",
    "# Drop rows with missing values in numerical columns for clean scaling\n",
    "titanic_num = titanic[num_cols].dropna()\n",
    "\n",
    "# Step 1: Standardize features using StandardScaler\n",
    "scaler = StandardScaler()\n",
    "titanic_scaled = scaler.fit_transform(titanic_num)\n",
    "\n",
    "# Convert back to DataFrame for easier viewing\n",
    "titanic_scaled_df = pd.DataFrame(titanic_scaled, columns=num_cols)\n",
    "\n",
    "# Step 2: Observe effect of standardization on data distribution\n",
    "\n",
    "# Plot original vs standardized feature distributions for 'age' and 'fare'\n",
    "fig, axs = plt.subplots(2, 2, figsize=(12, 8))\n",
    "\n",
    "# Original distributions\n",
    "sns.histplot(titanic_num['age'], kde=True, ax=axs[0,0], color='blue')\n",
    "axs[0,0].set_title('Original Age Distribution')\n",
    "\n",
    "sns.histplot(titanic_num['fare'], kde=True, ax=axs[0,1], color='blue')\n",
    "axs[0,1].set_title('Original Fare Distribution')\n",
    "\n",
    "# Standardized distributions\n",
    "sns.histplot(titanic_scaled_df['age'], kde=True, ax=axs[1,0], color='green')\n",
    "axs[1,0].set_title('Standardized Age Distribution')\n",
    "\n",
    "sns.histplot(titanic_scaled_df['fare'], kde=True, ax=axs[1,1], color='green')\n",
    "axs[1,1].set_title('Standardized Fare Distribution')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 2**: Min-Max Scaling\n",
    "\n",
    "- Step 1: Scale features to lie between 0 and 1 using MinMaxScaler.\n",
    "- Step 2: Compare with standardization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write your code from here\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "\n",
    "# Load Titanic dataset\n",
    "titanic = sns.load_dataset('titanic')\n",
    "\n",
    "# Select numerical columns\n",
    "num_cols = titanic.select_dtypes(include=['float64', 'int64']).columns.tolist()\n",
    "\n",
    "# Drop rows with missing values in numerical columns for clean scaling\n",
    "titanic_num = titanic[num_cols].dropna()\n",
    "\n",
    "# Standardization (for comparison)\n",
    "scaler_standard = StandardScaler()\n",
    "titanic_standardized = scaler_standard.fit_transform(titanic_num)\n",
    "titanic_standardized_df = pd.DataFrame(titanic_standardized, columns=num_cols)\n",
    "\n",
    "# Min-Max Scaling\n",
    "scaler_minmax = MinMaxScaler()\n",
    "titanic_minmax_scaled = scaler_minmax.fit_transform(titanic_num)\n",
    "titanic_minmax_df = pd.DataFrame(titanic_minmax_scaled, columns=num_cols)\n",
    "\n",
    "# Plotting original, standardized, and min-max scaled distributions for 'age' and 'fare'\n",
    "fig, axs = plt.subplots(3, 2, figsize=(12, 12))\n",
    "\n",
    "# Original distributions\n",
    "sns.histplot(titanic_num['age'], kde=True, ax=axs[0, 0], color='blue')\n",
    "axs[0, 0].set_title('Original Age Distribution')\n",
    "sns.histplot(titanic_num['fare'], kde=True, ax=axs[0, 1], color='blue')\n",
    "axs[0, 1].set_title('Original Fare Distribution')\n",
    "\n",
    "# Standardized distributions\n",
    "sns.histplot(titanic_standardized_df['age'], kde=True, ax=axs[1, 0], color='green')\n",
    "axs[1, 0].set_title('Standardized Age Distribution')\n",
    "sns.histplot(titanic_standardized_df['fare'], kde=True, ax=axs[1, 1], color='green')\n",
    "axs[1, 1].set_title('Standardized Fare Distribution')\n",
    "\n",
    "# Min-Max scaled distributions\n",
    "sns.histplot(titanic_minmax_df['age'], kde=True, ax=axs[2, 0], color='red')\n",
    "axs[2, 0].set_title('Min-Max Scaled Age Distribution')\n",
    "sns.histplot(titanic_minmax_df['fare'], kde=True, ax=axs[2, 1], color='red')\n",
    "axs[2, 1].set_title('Min-Max Scaled Fare Distribution')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 3**: Robust Scaling\n",
    "- Step 1: Scale features using RobustScaler, which is useful for data with outliers.\n",
    "- Step 2: Assess changes in data scaling compared to other scaling methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write your code from here\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler\n",
    "\n",
    "# Load Titanic dataset\n",
    "titanic = sns.load_dataset('titanic')\n",
    "\n",
    "# Select numerical columns\n",
    "num_cols = titanic.select_dtypes(include=['float64', 'int64']).columns.tolist()\n",
    "\n",
    "# Drop rows with missing values in numerical columns for clean scaling\n",
    "titanic_num = titanic[num_cols].dropna()\n",
    "\n",
    "# StandardScaler\n",
    "scaler_standard = StandardScaler()\n",
    "titanic_standardized = scaler_standard.fit_transform(titanic_num)\n",
    "titanic_standardized_df = pd.DataFrame(titanic_standardized, columns=num_cols)\n",
    "\n",
    "# MinMaxScaler\n",
    "scaler_minmax = MinMaxScaler()\n",
    "titanic_minmax_scaled = scaler_minmax.fit_transform(titanic_num)\n",
    "titanic_minmax_df = pd.DataFrame(titanic_minmax_scaled, columns=num_cols)\n",
    "\n",
    "# RobustScaler\n",
    "scaler_robust = RobustScaler()\n",
    "titanic_robust_scaled = scaler_robust.fit_transform(titanic_num)\n",
    "titanic_robust_df = pd.DataFrame(titanic_robust_scaled, columns=num_cols)\n",
    "\n",
    "# Plot distributions for 'fare' and 'age' to compare effects of scaling methods\n",
    "fig, axs = plt.subplots(4, 2, figsize=(12, 14))\n",
    "\n",
    "# Original distributions\n",
    "sns.histplot(titanic_num['age'], kde=True, ax=axs[0, 0], color='blue')\n",
    "axs[0, 0].set_title('Original Age Distribution')\n",
    "sns.histplot(titanic_num['fare'], kde=True, ax=axs[0, 1], color='blue')\n",
    "axs[0, 1].set_title('Original Fare Distribution')\n",
    "\n",
    "# Standardized distributions\n",
    "sns.histplot(titanic_standardized_df['age'], kde=True, ax=axs[1, 0], color='green')\n",
    "axs[1, 0].set_title('Standardized Age Distribution')\n",
    "sns.histplot(titanic_standardized_df['fare'], kde=True, ax=axs[1, 1], color='green')\n",
    "axs[1, 1].set_title('Standardized Fare Distribution')\n",
    "\n",
    "# Min-Max scaled distributions\n",
    "sns.histplot(titanic_minmax_df['age'], kde=True, ax=axs[2, 0], color='red')\n",
    "axs[2, 0].set_title('Min-Max Scaled Age Distribution')\n",
    "sns.histplot(titanic_minmax_df['fare'], kde=True, ax=axs[2, 1], color='red')\n",
    "axs[2, 1].set_title('Min-Max Scaled Fare Distribution')\n",
    "\n",
    "# Robust scaled distributions\n",
    "sns.histplot(titanic_robust_df['age'], kde=True, ax=axs[3, 0], color='purple')\n",
    "axs[3, 0].set_title('Robust Scaled Age Distributi_\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Selection Techniques:\n",
    "### Removing Highly Correlated Features:\n",
    "\n",
    "**Task 1**: Correlation Matrix\n",
    "- Step 1: Compute correlation matrix.\n",
    "- Step 2: Remove highly correlated features (correlation > 0.9)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# write your code from here\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_boston\n",
    "\n",
    "# Load dataset (Boston Housing for example)\n",
    "data = load_boston()\n",
    "df = pd.DataFrame(data.data, columns=data.feature_names)\n",
    "\n",
    "# Step 1: Compute correlation matrix\n",
    "corr_matrix = df.corr().abs()\n",
    "\n",
    "# Step 2: Select upper triangle of correlation matrix\n",
    "upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n",
    "\n",
    "# Find features with correlation greater than 0.9\n",
    "to_drop = [column for column in upper.columns if any(upper[column] > 0.9)]\n",
    "\n",
    "print(f\"Features to drop due to high correlation (> 0.9): {to_drop}\")\n",
    "\n",
    "# Drop highly correlated features\n",
    "df_reduced = df.drop(columns=to_drop)\n",
    "\n",
    "print(\"\\nRemaining features after removing highly correlated ones:\")\n",
    "print(df_reduced.columns.tolist())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Mutual Information & Variance Thresholds:\n",
    "\n",
    "**Task 2**: Mutual Information\n",
    "- Step 1: Compute mutual information between features and target.\n",
    "- Step 2: Retain features with high mutual information scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write your code from here\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "\n",
    "# Load example dataset\n",
    "data = load_breast_cancer()\n",
    "X = pd.DataFrame(data.data, columns=data.feature_names)\n",
    "y = data.target\n",
    "\n",
    "# Step 1: Compute mutual information between each feature and the target\n",
    "mi_scores = mutual_info_classif(X, y, discrete_features=False, random_state=42)\n",
    "\n",
    "# Create a DataFrame of features and their MI scores\n",
    "mi_df = pd.DataFrame({'Feature': X.columns, 'Mutual_Information': mi_scores})\n",
    "\n",
    "# Sort features by MI score descending\n",
    "mi_df = mi_df.sort_values(by='Mutual_Information', ascending=False)\n",
    "\n",
    "print(\"Mutual Information Scores:\")\n",
    "print(mi_df)\n",
    "\n",
    "# Step 2: Retain features with MI score above a threshold (e.g., > 0.01)\n",
    "threshold = 0.01\n",
    "selected_features = mi_df[mi_df['Mutual_Information'] > threshold]['Feature'].tolist()\n",
    "\n",
    "print(f\"\\nFeatures retained (MI > {threshold}):\")\n",
    "print(selected_features)\n",
    "\n",
    "# Reduced dataset with selected features\n",
    "X_reduced = X[selected_features]\n",
    "print(f\"\\nShape of reduced dataset: {X_reduced.shape}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 3**: Variance Threshold\n",
    "- Step 1: Implement VarianceThreshold to remove features with low variance.\n",
    "- Step 2: Analyze impact on feature space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write your code from here\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "\n",
    "# Load example dataset\n",
    "data = load_breast_cancer()\n",
    "X = pd.DataFrame(data.data, columns=data.feature_names)\n",
    "\n",
    "print(f\"Original number of features: {X.shape[1]}\")\n",
    "\n",
    "# Step 1: Remove features with low variance (threshold=0.01 here)\n",
    "selector = VarianceThreshold(threshold=0.01)\n",
    "X_reduced = selector.fit_transform(X)\n",
    "\n",
    "# Features kept mask\n",
    "features_kept = X.columns[selector.get_support()]\n",
    "\n",
    "print(f\"Number of features after VarianceThreshold: {X_reduced.shape[1]}\")\n",
    "print(\"Features retained:\")\n",
    "print(features_kept.tolist())\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
